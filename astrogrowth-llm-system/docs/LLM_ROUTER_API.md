# ğŸ¤– LLM Router API Documentation

## Table des MatiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [API Endpoints](#api-endpoints)
- [Types & Interfaces](#types--interfaces)
- [Exemples d'utilisation](#exemples-dutilisation)
- [Monitoring & Analytics](#monitoring--analytics)
- [Troubleshooting](#troubleshooting)

---

## Vue d'ensemble

Le **LLM Router** est un systÃ¨me intelligent de gestion multi-LLM avec:

- âœ… **3 tiers de failover** (OpenRouter â†’ Hugging Face â†’ Ollama)
- âœ… **Circuit breaker pattern** pour prÃ©venir cascading failures
- âœ… **Semantic caching** (80%+ rÃ©duction de coÃ»ts)
- âœ… **Task-based routing** (meilleur modÃ¨le par type de tÃ¢che)
- âœ… **Cost optimization** (modÃ¨les gratuits prioritaires)
- âœ… **Performance tracking** complet

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLM Router (llmRouter.ts)              â”‚
â”‚  - Multi-provider support                       â”‚
â”‚  - Intelligent failover                         â”‚
â”‚  - Circuit breaker integration                  â”‚
â”‚  - Semantic cache integration                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenRouter  â”‚ â”‚ HuggingFaceâ”‚ â”‚  Ollama   â”‚
â”‚   (Primary)  â”‚ â”‚ (Secondary)â”‚ â”‚ (Tertiary)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Endpoints

### 1. Complete LLM Request

**Endpoint:** `POST /api/trpc/llm.complete`

**Description:** ExÃ©cute une requÃªte LLM avec failover automatique

**Input:**
```typescript
{
  taskType: 'qualification' | 'content_generation' | 'verification' | 'analysis' | 'simple',
  messages: Array<{
    role: 'system' | 'user' | 'assistant',
    content: string
  }>,
  options?: {
    temperature?: number,      // 0.0 - 1.0 (default: 0.7)
    maxTokens?: number,        // Max tokens to generate (default: 1000)
    topP?: number,             // Nucleus sampling (default: 0.9)
    bypassCache?: boolean,     // Skip semantic cache (default: false)
    forceProvider?: string,    // Force specific provider
    forceModel?: string        // Force specific model
  }
}
```

**Output:**
```typescript
{
  content: string,                    // Generated text
  model: string,                      // Model used
  provider: string,                   // Provider used
  usage: {
    promptTokens?: number,
    completionTokens?: number,
    totalTokens?: number
  },
  metadata: {
    taskType: string,                 // Task type
    latencyMs: number,                // Response time
    cached: boolean,                  // From cache?
    fallbackAttempts: number,         // Number of failovers
    timestamp: string                 // ISO timestamp
  }
}
```

**Exemple:**
```bash
curl -X POST https://yourdomain.com/api/trpc/llm.complete \
  -H "Content-Type: application/json" \
  -H "Cookie: session=..." \
  -d '{
    "taskType": "qualification",
    "messages": [
      {
        "role": "system",
        "content": "Tu es un expert en qualification de leads B2B."
      },
      {
        "role": "user",
        "content": "Score ce lead (0-100): Restaurant Le Gourmet, 4.5/5, 200 avis"
      }
    ],
    "options": {
      "temperature": 0.3,
      "maxTokens": 500
    }
  }'
```

---

### 2. Analytics Summary

**Endpoint:** `GET /api/trpc/llmAnalytics.summary?days=7`

**Description:** RÃ©cupÃ¨re les statistiques d'utilisation LLM

**Query Parameters:**
- `days` (optional): Nombre de jours (default: 7)

**Output:**
```typescript
{
  totalCalls: number,
  totalTokens: number,
  totalCostCAD: number,
  totalSavingsCAD: number,        // From cache
  providers: {
    [provider: string]: {
      calls: number,
      tokens: number,
      estimatedCostCAD: number,
      avgLatencyMs: number,
      errorRate: number
    }
  },
  models: {
    [model: string]: {
      provider: string,
      calls: number,
      tokens: number,
      estimatedCostCAD: number,
      avgLatencyMs: number
    }
  },
  taskTypes: {
    [taskType: string]: {
      calls: number,
      tokens: number,
      estimatedCostCAD: number
    }
  },
  dailyUsage: Array<{
    date: string,
    calls: number,
    tokens: number,
    estimatedCostCAD: number
  }>,
  cacheMetrics: {
    hits: number,
    misses: number,
    hitRate: number,              // 0.0 - 1.0
    savingsCAD: number
  },
  circuitBreakerStatus: {
    [provider: string]: {
      state: 'CLOSED' | 'OPEN' | 'HALF_OPEN',
      failureCount: number,
      successCount: number,
      nextAttemptTime: number | null
    }
  }
}
```

**Exemple:**
```bash
curl -X GET "https://yourdomain.com/api/trpc/llmAnalytics.summary?days=30" \
  -H "Cookie: session=..."
```

---

### 3. Cost Forecast

**Endpoint:** `GET /api/trpc/llmAnalytics.forecast?horizonDays=30`

**Description:** PrÃ©vision des coÃ»ts LLM basÃ©e sur tendances

**Output:**
```typescript
{
  currentMonthlyCAD: number,
  projectedMonthlyCAD: number,
  dailyForecast: Array<{
    day: number,
    estimatedCostCAD: number
  }>
}
```

---

### 4. Cache Metrics

**Endpoint:** `GET /api/trpc/llmAnalytics.cacheMetrics`

**Description:** Statistiques du cache sÃ©mantique

**Output:**
```typescript
{
  hits: number,
  misses: number,
  hitRate: number,                // 0.0 - 1.0
  estimatedSavingsUSD: number
}
```

---

### 5. Cache Invalidation

**Endpoint:** `POST /api/trpc/llmAnalytics.cacheInvalidate`

**Description:** Invalider le cache (pattern optionnel)

**Input:**
```typescript
{
  pattern?: string    // Redis pattern (default: all)
}
```

**Output:**
```typescript
{
  invalidated: number   // Number of cache entries deleted
}
```

**Exemple:**
```bash
# Invalider tout le cache
curl -X POST https://yourdomain.com/api/trpc/llmAnalytics.cacheInvalidate \
  -H "Cookie: session=..." \
  -d '{}'

# Invalider pattern spÃ©cifique
curl -X POST https://yourdomain.com/api/trpc/llmAnalytics.cacheInvalidate \
  -H "Cookie: session=..." \
  -d '{ "pattern": "llm:cache:qualification:*" }'
```

---

### 6. Circuit Breaker Status

**Endpoint:** `GET /api/trpc/llmAnalytics.circuitBreakerStatus`

**Description:** Ã‰tat actuel des circuit breakers

**Output:**
```typescript
{
  [provider: string]: {
    state: 'CLOSED' | 'OPEN' | 'HALF_OPEN',
    failureCount: number,
    successCount: number,
    nextAttemptTime: number | null
  }
}
```

---

### 7. Circuit Breaker Reset

**Endpoint:** `POST /api/trpc/llmAnalytics.circuitBreakerReset`

**Description:** RÃ©initialiser circuit breaker(s)

**Input:**
```typescript
{
  provider?: string   // Specific provider or all
}
```

**Exemple:**
```bash
# Reset all
curl -X POST https://yourdomain.com/api/trpc/llmAnalytics.circuitBreakerReset \
  -H "Cookie: session=..." \
  -d '{}'

# Reset specific provider
curl -X POST https://yourdomain.com/api/trpc/llmAnalytics.circuitBreakerReset \
  -H "Cookie: session=..." \
  -d '{ "provider": "openrouter" }'
```

---

## Types & Interfaces

### TaskType

```typescript
enum TaskType {
  QUALIFICATION = 'qualification',       // Lead scoring
  CONTENT_GENERATION = 'content_generation',  // Marketing content
  VERIFICATION = 'verification',         // Quality checks
  ANALYSIS = 'analysis',                 // Data analysis
  SIMPLE = 'simple'                      // Basic tasks
}
```

### LLMProvider

```typescript
enum LLMProvider {
  OPENROUTER = 'openrouter',    // Primary tier
  HUGGINGFACE = 'huggingface',  // Secondary tier
  OLLAMA = 'ollama'             // Tertiary tier (local)
}
```

### Routing Strategy

```typescript
// Qualification: FREE models prioritized
QUALIFICATION: [
  { provider: 'openrouter', model: 'google/gemini-2.0-flash-exp:free' },  // 0$ CAD
  { provider: 'openrouter', model: 'meta-llama/llama-3.3-70b-instruct:free' },  // 0$ CAD
  { provider: 'huggingface', model: 'mistralai/Mistral-7B-Instruct-v0.2' },  // 0$ CAD
  { provider: 'ollama', model: 'llama3.2:3b' }  // 0$ CAD (local)
]

// Content Generation: Quality prioritized
CONTENT_GENERATION: [
  { provider: 'openrouter', model: 'anthropic/claude-sonnet-4' },  // Premium quality
  { provider: 'openrouter', model: 'openai/gpt-4o-mini' },  // Good fallback
  { provider: 'huggingface', model: 'mistralai/Mixtral-8x7B-Instruct-v0.1' },  // 0$ CAD
  { provider: 'ollama', model: 'llama3.2:3b' }  // 0$ CAD (emergency)
]

// Verification: FREE sufficient
VERIFICATION: [
  { provider: 'openrouter', model: 'google/gemini-2.0-flash-exp:free' },
  { provider: 'openrouter', model: 'meta-llama/llama-3.3-70b-instruct:free' },
  { provider: 'huggingface', model: 'HuggingFaceH4/zephyr-7b-beta' },
  { provider: 'ollama', model: 'phi3:mini' }
]
```

---

## Exemples d'utilisation

### Example 1: Qualification de Lead

```typescript
import { llmRouter, TaskType } from './services/llmRouter';

const qualifyLead = async (leadData: any) => {
  const response = await llmRouter.complete(
    TaskType.QUALIFICATION,
    [
      {
        role: 'system',
        content: 'Tu es un expert en qualification de leads B2B.'
      },
      {
        role: 'user',
        content: `Analyse ce lead et retourne un JSON avec: score (0-100), priority (high/medium/low), pain_points (array).

Lead: ${JSON.stringify(leadData)}`
      }
    ],
    {
      temperature: 0.3,  // Low temperature for consistent scoring
      maxTokens: 800
    }
  );

  return {
    qualification: JSON.parse(response.content),
    metadata: response.metadata
  };
};
```

### Example 2: GÃ©nÃ©ration de Contenu Marketing

```typescript
const generateMarketingPost = async (leadData: any) => {
  const response = await llmRouter.complete(
    TaskType.CONTENT_GENERATION,
    [
      {
        role: 'system',
        content: 'Tu es un expert en marketing de contenu pour PME quÃ©bÃ©coises.'
      },
      {
        role: 'user',
        content: `GÃ©nÃ¨re un post LinkedIn engageant pour:

Entreprise: ${leadData.businessName}
Type: ${leadData.businessType}
Ville: ${leadData.city}
Rating: ${leadData.googleRating}/5

Format JSON: { title, body, hashtags (3-5), cta }`
      }
    ],
    {
      temperature: 0.8,  // Higher for creativity
      maxTokens: 1500
    }
  );

  return {
    content: JSON.parse(response.content),
    provider: response.provider,
    model: response.model,
    cached: response.metadata.cached,
    costEstimate: calculateCost(response)
  };
};
```

### Example 3: Batch Processing

```typescript
const processLeadsBatch = async (leads: any[]) => {
  const requests = leads.map(lead => ({
    taskType: TaskType.QUALIFICATION,
    messages: [
      {
        role: 'user',
        content: `Score this lead (0-100): ${JSON.stringify(lead)}`
      }
    ]
  }));

  // Process in parallel with concurrency control
  const results = await llmRouter.batchComplete(requests, 5);

  return results.map((r, i) => ({
    lead_id: leads[i].id,
    score: parseInt(r.content),
    provider: r.provider,
    cached: r.metadata.cached
  }));
};
```

---

## Monitoring & Analytics

### Dashboard Metrics Ã  Surveiller

1. **Cache Hit Rate** (target: > 60%)
   - Indique l'efficacitÃ© du cache sÃ©mantique
   - Hit rate faible = opportunitÃ© d'optimisation

2. **Provider Distribution**
   - IdÃ©al: MajoritÃ© sur modÃ¨les gratuits (OpenRouter free, HuggingFace)
   - Si trop de Claude Sonnet 4 = coÃ»ts Ã©levÃ©s

3. **Circuit Breaker State**
   - Tous CLOSED = healthy
   - OPEN = provider down, investigate

4. **Average Latency**
   - < 2s = excellent
   - 2-5s = acceptable
   - > 5s = investigate (provider slow?)

5. **Cost Trends**
   - Comparer actual vs forecast
   - Alerter si > budget mensuel

---

## Troubleshooting

### Issue: "Circuit breaker is OPEN"

**Cause:** Trop d'Ã©checs consÃ©cutifs sur un provider

**Solution:**
```bash
# Check status
curl https://yourdomain.com/api/trpc/llmAnalytics.circuitBreakerStatus

# Reset specific provider
curl -X POST https://yourdomain.com/api/trpc/llmAnalytics.circuitBreakerReset \
  -d '{ "provider": "openrouter" }'
```

### Issue: Cache non performant

**VÃ©rifier:**
```bash
curl https://yourdomain.com/api/trpc/llmAnalytics.cacheMetrics
```

**Si hit rate < 40%:**
- VÃ©rifier que `bypassCache: false` par dÃ©faut
- VÃ©rifier TTL (peut-Ãªtre trop court)
- Analyzer si queries sont trop variables

### Issue: CoÃ»ts trop Ã©levÃ©s

**Analyser:**
```bash
curl https://yourdomain.com/api/trpc/llmAnalytics.summary?days=30
```

**Optimisations:**
1. VÃ©rifier distribution modÃ¨les (prioritÃ© aux gratuits?)
2. Augmenter cache TTL pour queries similaires
3. RÃ©duire `maxTokens` si possible
4. Utiliser `temperature: 0.3` pour tasks dÃ©terministes

### Issue: Latence Ã©levÃ©e

**Diagnostiquer:**
- Check `metadata.latencyMs` dans responses
- Analyser `fallbackAttempts` (> 2 = problÃ¨me?)
- VÃ©rifier circuit breaker status
- Test latency par provider

**Optimiser:**
- Warmer le cache avec queries communes
- Utiliser batch processing pour volume
- ConsidÃ©rer augmenter concurrency limit

---

## Best Practices

### âœ… DO

- âœ… Utiliser semantic cache par dÃ©faut
- âœ… Choisir le bon `TaskType` (routing optimal)
- âœ… Monitorer les metrics quotidiennement
- âœ… Warmer le cache avec queries frÃ©quentes
- âœ… Utiliser batch processing pour volume
- âœ… Logger les failover attempts

### âŒ DON'T

- âŒ Bypass cache sans raison
- âŒ Force provider (sauf debug)
- âŒ Ignorer circuit breaker warnings
- âŒ Utiliser temperature > 0.9 (coÃ»teux)
- âŒ DÃ©passer rate limits providers

---

## Support

Questions? Issues?

- ğŸ“§ Email: support@astrogrowth.ca
- ğŸ› GitHub Issues: [Link]
- ğŸ“š Docs: https://docs.astrogrowth.ca

---

**Construit avec â¤ï¸ par l'Ã©quipe AstroGrowth**
